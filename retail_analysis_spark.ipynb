{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Retail Transaction Analysis Backend (Spark + Flask + Ngrok)\n",
                "\n",
                "This notebook runs the backend analysis service.\n",
                "1. Be sure to run all cells.\n",
                "2. Enter your Ngrok Authtoken when prompted (or hardcode it).\n",
                "3. Copy the URL printed at the end (e.g., `http://xxxx.ngrok-free.app`) to your local dashboard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install dependencies\n",
                "!pip install pyspark flask flask-cors pyngrok"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "b58c75d5",
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from pyspark.sql import SparkSession\n",
                "from pyspark.ml.fpm import FPGrowth\n",
                "from pyspark.sql.functions import col, collect_set, struct, to_date, to_timestamp, sum as _sum, count, countDistinct, desc, date_format, coalesce, expr\n",
                "from flask import Flask, request, jsonify\n",
                "from flask_cors import CORS\n",
                "from pyngrok import ngrok\n",
                "import shutil\n",
                "\n",
                "# Initialize Spark\n",
                "spark = SparkSession.builder \\\n",
                "    .appName(\"RetailAnalysis\") \\\n",
                "    .master(\"local[*]\") \\\n",
                "    .config(\"spark.driver.memory\", \"4g\") \\\n",
                "    .config(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\") \\\n",
                "    .getOrCreate()\n",
                "\n",
                "print(\"Spark Initialized\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9a565780",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Analysis Functions\n",
                "\n",
                "def standardize_columns(df):\n",
                "    # aggressively standardize columns\n",
                "    # 1. Strip whitespace and lower case map\n",
                "    original_cols = df.columns\n",
                "    col_map = {c.strip().lower(): c for c in original_cols}\n",
                "    \n",
                "    # Rename logic\n",
                "    if 'price' in col_map and 'unitprice' not in col_map:\n",
                "        df = df.withColumnRenamed(col_map['price'], 'UnitPrice')\n",
                "        \n",
                "    if 'invoice' in col_map and 'invoiceno' not in col_map:\n",
                "        df = df.withColumnRenamed(col_map['invoice'], 'InvoiceNo')\n",
                "        \n",
                "    if 'invoicedate' in col_map and col_map['invoicedate'] != 'InvoiceDate':\n",
                "        df = df.withColumnRenamed(col_map['invoicedate'], 'InvoiceDate')\n",
                "        \n",
                "    return df\n",
                "\n",
                "def check_columns(df, required):\n",
                "    missing = [c for c in required if c not in df.columns]\n",
                "    if missing:\n",
                "        return f\"Missing columns: {', '.join(missing)}. Found: {', '.join(df.columns)}\"\n",
                "    return None\n",
                "\n",
                "def get_common_df(df_path):\n",
                "    # Helper to load and standardize DF\n",
                "    try:\n",
                "        df = spark.read.csv(df_path, header=True, inferSchema=True)\n",
                "        df = standardize_columns(df)\n",
                "        return df, None\n",
                "    except Exception as e:\n",
                "        return None, str(e)\n",
                "\n",
                "def analyze_basket(df_path, min_support=0.01, min_confidence=0.1):\n",
                "    df, err = get_common_df(df_path)\n",
                "    if err: return None, err\n",
                "    \n",
                "    item_col = 'Description'\n",
                "    if 'Description' not in df.columns and 'StockCode' in df.columns:\n",
                "        item_col = 'StockCode'\n",
                "        \n",
                "    err = check_columns(df, ['InvoiceNo', item_col])\n",
                "    if err: return None, err\n",
                "    \n",
                "    df = df.dropna(subset=['InvoiceNo', item_col])\n",
                "    transactions = df.groupBy(\"InvoiceNo\").agg(collect_set(item_col).alias(\"items\"))\n",
                "    \n",
                "    fpGrowth = FPGrowth(itemsCol=\"items\", minSupport=min_support, minConfidence=min_confidence)\n",
                "    model = fpGrowth.fit(transactions)\n",
                "    \n",
                "    rules = model.associationRules.sort(col(\"lift\").desc()).limit(50)\n",
                "    rules_pdf = rules.toPandas()\n",
                "    return rules_pdf.to_dict(orient='records'), None\n",
                "\n",
                "def analyze_sales(df_path):\n",
                "    df, err = get_common_df(df_path)\n",
                "    if err: return None, err\n",
                "    \n",
                "    err = check_columns(df, ['InvoiceDate', 'Quantity', 'UnitPrice'])\n",
                "    if err: return None, err\n",
                "    df = df.dropna(subset=['InvoiceDate', 'Quantity', 'UnitPrice'])\n",
                "    \n",
                "    if 'TotalAmount' not in df.columns:\n",
                "        df = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
                "    \n",
                "    # Robust Date Parsing\n",
                "    df = df.withColumn(\"ParsedDate\", \n",
                "        coalesce(\n",
                "            expr(\"try_to_timestamp(InvoiceDate, 'dd-MM-yyyy HH:mm')\"),\n",
                "            expr(\"try_to_timestamp(InvoiceDate, 'M/d/yyyy H:m')\"),\n",
                "            expr(\"try_to_timestamp(InvoiceDate, 'dd/MM/yyyy HH:mm')\"),\n",
                "            expr(\"try_to_timestamp(InvoiceDate, 'yyyy-MM-dd HH:mm:ss')\"),\n",
                "            expr(\"try_to_timestamp(InvoiceDate, 'yyyy-MM-dd')\"),\n",
                "            expr(\"try_to_timestamp(InvoiceDate, 'M/d/yyyy')\"),\n",
                "            expr(\"try_to_timestamp(InvoiceDate, 'dd/MM/yyyy')\")\n",
                "        )\n",
                "    )\n",
                "    df = df.withColumn(\"Date\", to_date(col(\"ParsedDate\")))\n",
                "    df = df.filter(col(\"Date\").isNotNull())\n",
                "    \n",
                "    daily_sales = df.groupBy(\"Date\").agg(_sum(\"TotalAmount\").alias(\"DailySales\")).orderBy(\"Date\")\n",
                "    pdf = daily_sales.toPandas()\n",
                "    if pdf.empty: return None, \"No valid dates found.\"\n",
                "    \n",
                "    pdf['Date'] = pdf['Date'].astype(str)\n",
                "    return pdf.to_dict(orient='records'), None\n",
                "\n",
                "def analyze_summary(df_path):\n",
                "    df, err = get_common_df(df_path)\n",
                "    if err: return None, err\n",
                "    \n",
                "    required = ['Quantity', 'UnitPrice', 'InvoiceNo', 'Customer ID']\n",
                "    # Customer ID is optional, handle nicely if missing\n",
                "    \n",
                "    if 'TotalAmount' not in df.columns:\n",
                "        df = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
                "        \n",
                "    # Aggregations\n",
                "    total_revenue = df.agg(_sum(\"TotalAmount\")).collect()[0][0]\n",
                "    total_transactions = df.select(\"InvoiceNo\").distinct().count()\n",
                "    total_items = df.agg(_sum(\"Quantity\")).collect()[0][0]\n",
                "    \n",
                "    total_customers = 0\n",
                "    if \"Customer ID\" in df.columns:\n",
                "         total_customers = df.select(\"Customer ID\").distinct().count()\n",
                "    elif \"CustomerID\" in df.columns:\n",
                "         total_customers = df.select(\"CustomerID\").distinct().count()\n",
                "         \n",
                "    return {\n",
                "        \"total_revenue\": total_revenue or 0,\n",
                "        \"total_transactions\": total_transactions,\n",
                "        \"total_items\": total_items or 0,\n",
                "        \"total_customers\": total_customers\n",
                "    }, None\n",
                "\n",
                "def analyze_top_products(df_path):\n",
                "    df, err = get_common_df(df_path)\n",
                "    if err: return None, err\n",
                "    \n",
                "    if 'TotalAmount' not in df.columns:\n",
                "        df = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
                "        \n",
                "    item_col = 'Description'\n",
                "    if 'Description' not in df.columns: item_col = 'StockCode'\n",
                "    \n",
                "    top = df.groupBy(item_col).agg(_sum(\"TotalAmount\").alias(\"Revenue\")).orderBy(desc(\"Revenue\")).limit(5)\n",
                "    return top.toPandas().to_dict(orient='records'), None\n",
                "\n",
                "def analyze_by_country(df_path):\n",
                "    df, err = get_common_df(df_path)\n",
                "    if err: return None, err\n",
                "    \n",
                "    if 'Country' not in df.columns: return [], \"'Country' column missing\"\n",
                "    \n",
                "    if 'TotalAmount' not in df.columns:\n",
                "        df = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"UnitPrice\"))\n",
                "        \n",
                "    countries = df.groupBy(\"Country\").agg(_sum(\"TotalAmount\").alias(\"Revenue\")).orderBy(desc(\"Revenue\")).limit(10)\n",
                "    return countries.toPandas().to_dict(orient='records'), None"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "ab7d2af4",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Flask API\n",
                "app = Flask(__name__)\n",
                "CORS(app, resources={r\"/*\": {\"origins\": \"*\", \"allow_headers\": [\"Content-Type\", \"ngrok-skip-browser-warning\"]}})\n",
                "\n",
                "UPLOAD_FOLDER = \"/content/uploads\"\n",
                "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
                "CURRENT_FILE = None\n",
                "\n",
                "@app.route('/')\n",
                "def home():\n",
                "    return jsonify({\"status\": \"running\"})\n",
                "\n",
                "@app.route('/upload', methods=['POST'])\n",
                "def upload_file():\n",
                "    global CURRENT_FILE\n",
                "    if os.path.exists(UPLOAD_FOLDER): shutil.rmtree(UPLOAD_FOLDER)\n",
                "    os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
                "    \n",
                "    if 'file' not in request.files: return jsonify({\"error\": \"No file part\"}), 400\n",
                "    file = request.files['file']\n",
                "    if file.filename == '': return jsonify({\"error\": \"No selected file\"}), 400\n",
                "    \n",
                "    filepath = os.path.join(UPLOAD_FOLDER, \"transaction_data.csv\")\n",
                "    file.save(filepath)\n",
                "    CURRENT_FILE = filepath\n",
                "    return jsonify({\"message\": \"File uploaded successfully\", \"path\": filepath})\n",
                "\n",
                "@app.route('/analyze/basket', methods=['GET'])\n",
                "def get_basket_analysis():\n",
                "    global CURRENT_FILE\n",
                "    if not CURRENT_FILE: return jsonify({\"error\": \"No file uploaded\"}), 400\n",
                "    min_sup = float(request.args.get('min_support', 0.01))\n",
                "    min_conf = float(request.args.get('min_confidence', 0.1))\n",
                "    results, error = analyze_basket(CURRENT_FILE, min_sup, min_conf)\n",
                "    if error: return jsonify({\"error\": error}), 500\n",
                "    return jsonify(results)\n",
                "\n",
                "@app.route('/analyze/sales', methods=['GET'])\n",
                "def get_sales_analysis():\n",
                "    global CURRENT_FILE\n",
                "    if not CURRENT_FILE: return jsonify({\"error\": \"No file uploaded\"}), 400\n",
                "    results, error = analyze_sales(CURRENT_FILE)\n",
                "    if error: return jsonify({\"error\": error}), 500\n",
                "    return jsonify(results)\n",
                "    \n",
                "@app.route('/analyze/summary', methods=['GET'])\n",
                "def get_summary():\n",
                "    global CURRENT_FILE\n",
                "    if not CURRENT_FILE: return jsonify({\"error\": \"No file uploaded\"}), 400\n",
                "    results, error = analyze_summary(CURRENT_FILE)\n",
                "    if error: return jsonify({\"error\": error}), 500\n",
                "    return jsonify(results)\n",
                "    \n",
                "@app.route('/analyze/top_products', methods=['GET'])\n",
                "def get_top_products():\n",
                "    global CURRENT_FILE\n",
                "    if not CURRENT_FILE: return jsonify({\"error\": \"No file uploaded\"}), 400\n",
                "    results, error = analyze_top_products(CURRENT_FILE)\n",
                "    if error: return jsonify({\"error\": error}), 500\n",
                "    return jsonify(results)\n",
                "    \n",
                "@app.route('/analyze/by_country', methods=['GET'])\n",
                "def get_by_country():\n",
                "    global CURRENT_FILE\n",
                "    if not CURRENT_FILE: return jsonify({\"error\": \"No file uploaded\"}), 400\n",
                "    results, error = analyze_by_country(CURRENT_FILE)\n",
                "    if error: return jsonify({\"error\": error}), 500\n",
                "    return jsonify(results)\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "483c201f",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Start Ngrok and Flask\n",
                "NGROK_AUTH_TOKEN = \"36ezKIxPMhSIslCTqCaIAV8Od8M_5LfejcBq9MtTBb2ZZ7sGJ\"\n",
                "if NGROK_AUTH_TOKEN == \"ENTER_YOUR_TOKEN_HERE\":\n",
                "    print(\"WARNING: Please set your NGROK_AUTH_TOKEN in the code cell above!\")\n",
                "else:\n",
                "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
                "    public_url = ngrok.connect(5000)\n",
                "    print(f\"\\n\\n>>> PUBLIC URL: {public_url} <<<\\n\\n\")\n",
                "    app.run(port=5000)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.9"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
